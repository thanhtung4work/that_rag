[INFO]: Using microsoft/Phi-3.5-mini-instruct
Query Retrieval Augmented Generation (RAG) is a research paradigm that enhances the capabilities of Large Language Models (LLMs) by integrating external knowledge sources during the generation process. The main stages of RAG include:

1. Retrieval: In this stage, relevant information is sourced from external databases or knowledge bases to supplement the original query. The goal is to provide the LLM with additional context and knowledge to generate more accurate and informed responses.

2. Generation: The retrieved information is combined with the original query, and the LLM generates a response based on the enriched context. This allows the model to produce answers that are not only contextually relevant but also up-to-date and accurate.

3. Augmentation: The process of retrieval and generation is iterative, allowing the LLM to refine its response by incorporating additional information or adjusting its output based on feedback.

RAG can be categorized into three main paradigms:

- Naive RAG: This early methodology consists of three parts: indexing, retrieval, and generation. It relies on a chain-like structure, where the retrieved information is directly fed into the generation process.

- Advanced RAG: This paradigm introduces multiple optimization strategies around pre-retrieval and post-retrieval processes. It maintains a similar structure to Naive RAG but incorporates additional techniques to improve the retrieval and generation processes.

- Modular RAG: Building upon the previous paradigms, Modular RAG offers greater flexibility by introducing specific functional modules and replacing existing ones. It features methods such as iterative and adaptive retrieval, allowing for a more dynamic and efficient RAG process.

In summary, RAG is a research paradigm that aims to bridge the gap between LLMs and up-to-date information by integrating external knowledge sources during the generation process. It has evolved through three main paradigms, each with its own strengths and weaknesses, and continues to be an active area of research in the field of artificial intelligence.
[{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'Now use the following context items to answer the user query:\n    - This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.”On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: • In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5 [cs. CL] 27 Mar 2024\n- II. OVERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG.\n- 4 Fig.3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval. Pre-retrieval process. In this stage, the primary focus is on optimizing the indexing structure and the original query.\n- A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the\n- 2 Fig.1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques.advanced RAG, and modular RAG. This review contex- tualizes the broader scope of RAG research within the landscape of LLMs. •We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework. •\nRelevant passages: <extract relevant passages from the context here>\n    User query: what is RAG\n    Answer:\n    '}]
